<!DOCTYPE html>
<html lang="ko" xmlns:th="http://www.thymeleaf.org">

<th:block>
    <h2>Chapter4. Multimodality API - Images and Vision</h2><br>
    <h4>1. Multimodality API</h4><br>
    <p>인간은 다양한 데이터 입력 방식을 통해 동시에 지식을 처리합니다. 우리가 학습하는 방식과 경험은 모두 다중 모드입니다. 시각, 청각, 텍스트만 있는 것이 아닙니다.

        이러한 원칙과는 달리, 머신 러닝은 종종 단일 모달리티를 처리하도록 맞춤화된 특화된 모델에 집중되었습니다. 예를 들어, 텍스트-음성 변환이나 음성-텍스트 변환과 같은 작업을 위한 오디오 모델과 객체 감지 및 분류와 같은 작업을 위한 컴퓨터 비전 모델을 개발했습니다.

        그러나 새로운 멀티모달 대규모 언어 모델이 등장하기 시작했습니다. OpenAI의 GPT-4o, Google의 Vertex AI Gemini 1.5, Anthropic의 Claude3, 그리고 오픈소스 솔루션인 Llama3.2, LLaVA, BakLLaVA 등이 그 예입니다. 이러한 솔루션들은 텍스트, 이미지, 오디오, 비디오 등 여러 입력을 수용하고 이러한 입력을 통합하여 텍스트 응답을 생성할 수 있습니다.</p>
    <p>다중 모달리티는 텍스트, 이미지, 오디오 및 기타 데이터 형식을 포함한 다양한 소스의 정보를 동시에 이해하고 처리할 수 있는 모델의 능력을 말합니다.
       Spring AI Message API는 멀티모달 LLM을 지원하는 데 필요한 모든 추상화를 제공합니다.</p>
    <img th:src="@{/imgs/multimodality.png}" width="600px;"><br><a href="https://docs.spring.io/spring-ai/reference/api/multimodality.html">출처:docs.springai.io</a><br><br>
    <p>UserMessage content필드는 주로 텍스트 입력에 사용되는 반면, 선택 media필드는 이미지, 오디오, 비디오 등 다양한 모달리티의 콘텐츠를 하나 이상 추가할 수 있도록 합니다. 이 필드는 MimeType모달리티 유형을 지정합니다. 사용된 LLM에 따라 Media데이터 필드는 원시 미디어 콘텐츠( Resource객체) 또는 URI콘텐츠에 대한 데이터(a)가 될 수 있습니다.</p><br>
    <p>Spring AI는 다음과 같은 채팅 모델에 대해 멀티모달 지원을 제공합니다.</p>
    <ul>
        <li>Anthropic Claude 3</li>
        <li>AWS Bedrock Converse</li>
        <li>Azure Open AI (e.g. GPT-4o models)</li>
        <li>Mistral AI (e.g. Mistral Pixtral models)</li>
        <li>Ollama (e.g. LLaVA, BakLLaVA, Llama3.2 models)</li>
        <li>OpenAI (e.g. GPT-4 and GPT-4o models)</li>
        <li>Vertex AI Gemini (e.g. gemini-1.5-pro-001, gemini-1.5-flash-001 models)</li>
    </ul><br>
    <h4>2. Analyze images</h4><br>
    <a href="https://platform.openai.com/docs/guides/images-vision?api-mode=responses">출처:platform.openai.com</a><br><br>
    <pre style="border:1px solid cornflowerblue">

        Message systemMessage  = systemPrompt.createMessage();

            public Flux&lt;String&gt; imageAnalysis(String question, String contentType, byte[] bytes) {

                Message systemMessage  = systemPrompt.createMessage();

                Media media = Media.builder()
                        .mimeType(MimeType.valueOf(contentType))
                        .data(new ByteArrayResource(bytes))
                        .build();
                UserMessage userMessage = UserMessage.builder()
                        .text(question)
                        .media(media)
                        .build();

                return chatClient.prompt()
                        .messages(userMessage,systemMessage)
                        .stream()
                        .content();
            }

    </pre><br>

    <h4>3. Image generation</h4><br>
    <a href="https://platform.openai.com/docs/guides/image-generation?image-generation-model=gpt-image-1">출처:platform.openai.com</a><br><br>
    <pre style="border:1px solid cornflowerblue">

            public String generateImageToText(String description) {
                return generateImage(description, "b64_json")
                        .getResult()
                        .getOutput()
                        .getB64Json();
            }
            private ImageResponse generateImage(String description, String format) {

                ImageMessage imageMessage = new ImageMessage(description);
                OpenAiImageOptions imageOptions = OpenAiImageOptions.builder()
                        .model("dall-e-3")
                        .responseFormat(format)
                        .width(1024)
                        .height(1024)
                        .N(1)
                        .build();
                List&lt;ImageMessage&gt; imageMessageList = List.of(imageMessage);
                ImagePrompt imagePrompt = new ImagePrompt(imageMessageList, imageOptions);

                return imageModel.call(imagePrompt);
            }
    </pre>

</th:block>

</html>